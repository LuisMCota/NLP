{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApF3hBdaavwX"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpr81e7bc0G4",
        "outputId": "e826eaf6-c54a-4dbd-c372-57cb5ecad3c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9-wRHB2oHjh",
        "outputId": "c6115f18-b838-484e-d348-360ea705aacf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/dianalaura/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/dianalaura/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qp5sEXvpazmB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import emoji\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from unicodedata import normalize\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sZoYUMv6cvYc",
        "outputId": "df77e1e7-1a66-4190-9648-cdc8916e37a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Title</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>One of the best game music soundtracks - for a...</td>\n",
              "      <td>Despite the fact that I have only played a sma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Batteries died within a year ...</td>\n",
              "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>works fine, but Maha Energy is better</td>\n",
              "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>Great for the non-audiophile</td>\n",
              "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>DVD Player crapped out after one year</td>\n",
              "      <td>I also began having the incorrect disc problem...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity                                              Title  \\\n",
              "0         2  One of the best game music soundtracks - for a...   \n",
              "1         1                   Batteries died within a year ...   \n",
              "2         2              works fine, but Maha Energy is better   \n",
              "3         2                       Great for the non-audiophile   \n",
              "4         1              DVD Player crapped out after one year   \n",
              "\n",
              "                                                Text  \n",
              "0  Despite the fact that I have only played a sma...  \n",
              "1  I bought this charger in Jul 2003 and it worke...  \n",
              "2  Check out Maha Energy's website. Their Powerex...  \n",
              "3  Reviewed quite a bit of the combo players and ...  \n",
              "4  I also began having the incorrect disc problem...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset = pd.read_csv('/Users/dianalaura/Documents/MARIO PROJECT/marioproject/archivos/test.csv')\n",
        "test_dataset.columns = ['Polarity', 'Title', 'Text']\n",
        "test_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hurjbDBLYede",
        "outputId": "14d5fcc3-1d1c-4184-e58e-a1e2fa781d52"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Title</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>The best soundtrack ever to anything.</td>\n",
              "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Amazing!</td>\n",
              "      <td>This soundtrack is my favorite music of all ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Excellent Soundtrack</td>\n",
              "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
              "      <td>If you've played the game, you know how divine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>an absolute masterpiece</td>\n",
              "      <td>I am quite sure any of you actually taking the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Polarity                                              Title  \\\n",
              "0         2              The best soundtrack ever to anything.   \n",
              "1         2                                           Amazing!   \n",
              "2         2                               Excellent Soundtrack   \n",
              "3         2  Remember, Pull Your Jaw Off The Floor After He...   \n",
              "4         2                            an absolute masterpiece   \n",
              "\n",
              "                                                Text  \n",
              "0  I'm reading a lot of reviews saying that this ...  \n",
              "1  This soundtrack is my favorite music of all ti...  \n",
              "2  I truly like this soundtrack and I enjoy video...  \n",
              "3  If you've played the game, you know how divine...  \n",
              "4  I am quite sure any of you actually taking the...  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = pd.read_csv('/Users/dianalaura/Documents/MARIO PROJECT/marioproject/archivos/train.csv')\n",
        "train_dataset.columns = ['Polarity', 'Title', 'Text']\n",
        "train_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cCXTtrtHnYey"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_clean_dataset(df):\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        text = re.sub(r'http\\S+|www\\S+|@\\w+|#', '', text)\n",
        "        text = normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        text = text.lower()\n",
        "        text = emoji.replace_emoji(text, replace='')\n",
        "        text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    df['Review'] = df['Title'].fillna('') + \" \" + df['Text'].fillna('')\n",
        "\n",
        "    df['Review'] = df['Review'].apply(preprocess_text)\n",
        "\n",
        "    df = df.drop_duplicates(subset='Review')\n",
        "\n",
        "    df = df[['Polarity', 'Review']]\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "eMAo2Ur_ogF3",
        "outputId": "9579a565-c992-4571-c17c-5d06a5910a8b"
      },
      "outputs": [],
      "source": [
        "test_dataset = preprocess_and_clean_dataset(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "Nq_oJJSFngf0",
        "outputId": "317f8df3-1dd4-413a-cffa-14d69d485bef"
      },
      "outputs": [],
      "source": [
        "train_dataset = preprocess_and_clean_dataset(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DIANA - The Generative Model I\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Perform pre processing (as needed) to train the phrase generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columnas disponibles en el dataset:\n",
            "Index(['Polarity', 'Review', 'inverted_polarity'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(\"Columns available in the dataset:\")\n",
        "print(train_dataset.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dataset(dataset):\n",
        "    if 'Review' not in dataset.columns or 'Polarity' not in dataset.columns:\n",
        "        raise ValueError(\"The dataset does not contain the necessary columns: 'Review' and 'Polarity'\")\n",
        "    \n",
        "    # Reverse polarity\n",
        "    dataset['inverted_polarity'] = dataset['Polarity'].map({1: 2, 2: 1})\n",
        "    dataset['target_text'] = dataset['Review'].apply(lambda x: invert_sentiment_text(x))  # Reverse text\n",
        "    return dataset\n",
        "\n",
        "def invert_sentiment_text(text):\n",
        "    replacements = {\n",
        "        \"good\": \"bad\",\n",
        "        \"excellent\": \"terrible\",\n",
        "        \"happy\": \"sad\",\n",
        "        # Add more replacements as needed\n",
        "    }\n",
        "    for word, replacement in replacements.items():\n",
        "        text = text.replace(word, replacement)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "train_dataset = prepare_dataset(train_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              Review  \\\n",
            "0  best soundtrack ever anyth im read lot review ...   \n",
            "1  amaz soundtrack favorit music time hand intens...   \n",
            "2  excel soundtrack truli like soundtrack enjoy v...   \n",
            "3  rememb pull jaw floor hear youv play game know...   \n",
            "4  absolut masterpiec quit sure actual take time ...   \n",
            "\n",
            "                                         target_text  Polarity  \\\n",
            "0  best soundtrack ever anyth im read lot review ...         2   \n",
            "1  amaz soundtrack favorit music time hand intens...         2   \n",
            "2  excel soundtrack truli like soundtrack enjoy v...         2   \n",
            "3  rememb pull jaw floor hear youv play game know...         2   \n",
            "4  absolut masterpiec quit sure actual take time ...         2   \n",
            "\n",
            "   inverted_polarity  \n",
            "0                  1  \n",
            "1                  1  \n",
            "2                  1  \n",
            "3                  1  \n",
            "4                  1  \n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[['Review', 'target_text', 'Polarity', 'inverted_polarity']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivo procesado guardado como 'train_preprocessed_inverted.csv'\n"
          ]
        }
      ],
      "source": [
        "train_dataset.to_csv(\"train_preprocessed_inverted.csv\", index=False)\n",
        "print(\"Processed file saved as 'train_preprocessed_inverted.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create an encoder decoder generative model to convert the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Prepare the Text (Tokenization and Sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# ParAMETERS\n",
        "max_vocab_size = 20000  # Maximum number of words in the vocabulary\n",
        "max_seq_length = 100  # Maximum length of sequences\n",
        "\n",
        "# Tokenizer for texts\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_dataset['Review'])  # Fit to original text\n",
        "\n",
        "# Convert text to integer sequences\n",
        "encoder_input_sequences = tokenizer.texts_to_sequences(train_dataset['Review'])\n",
        "decoder_input_sequences = tokenizer.texts_to_sequences(train_dataset['target_text'])\n",
        "\n",
        "# Padding (padding to have all sequences the same length)\n",
        "encoder_input_data = pad_sequences(encoder_input_sequences, maxlen=max_seq_length, padding='post')\n",
        "decoder_input_data = pad_sequences(decoder_input_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Create tags (expected decoder output)\n",
        "decoder_target_data = np.zeros_like(decoder_input_data)\n",
        "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]  # Shift so that the output is aligned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Define the Encoder-Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,140,000</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m2,560,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m2,560,000\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m394,240\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,      │    \u001b[38;5;34m394,240\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,       │  \u001b[38;5;34m5,140,000\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│                     │ \u001b[38;5;34m20000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,048,480</span> (42.15 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,048,480\u001b[0m (42.15 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,048,480</span> (42.15 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,048,480\u001b[0m (42.15 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 128  # Dimension of the embedding vector (word embeddings)\n",
        "latent_dim = 256  #Latent space dimension (LSTM)\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_seq_length,))\n",
        "encoder_embedding = Embedding(max_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_seq_length,))\n",
        "decoder_embedding = Embedding(max_vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(max_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Complete model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compilation\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m21806/44980\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21:21:31\u001b[0m 3s/step - accuracy: 0.5943 - loss: 7.0658"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to generate inverted text\n",
        "def generate_inverted_text(input_text):\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=max_seq_length, padding='post')\n",
        "    \n",
        "    states_value = encoder_lstm.predict(input_sequence)\n",
        "    target_sequence = np.zeros((1, max_seq_length))\n",
        "    target_sequence[0, 0] = tokenizer.word_index['<start>']  # Startup token\n",
        "    \n",
        "    stop_condition = False\n",
        "    generated_text = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_lstm.predict([target_sequence] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer.index_word[sampled_token_index]\n",
        "        \n",
        "        if sampled_word == '<end>' or len(generated_text) > max_seq_length:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            generated_text += ' ' + sampled_word\n",
        "        \n",
        "        target_sequence = np.zeros((1, max_seq_length))\n",
        "        target_sequence[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "    \n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build the proper datasets to create the Generative Model using the polarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to generate text using the trained model\n",
        "def generate_text(input_text, tokenizer, encoder_model, decoder_model, max_seq_length):\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=max_seq_length, padding='post')\n",
        "    \n",
        "    # Get encoder states\n",
        "    states_value = encoder_model.predict(input_sequence)\n",
        "    \n",
        "    # Initial sequence for the decoder\n",
        "    target_sequence = np.zeros((1, max_seq_length))\n",
        "    target_sequence[0, 0] = tokenizer.word_index['<start>']  # Startup token\n",
        "\n",
        "    stop_condition = False\n",
        "    generated_text = \"\"\n",
        "    \n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_sequence] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer.index_word.get(sampled_token_index, \"\")\n",
        "        \n",
        "        if sampled_word == \"<end>\" or len(generated_text.split()) > max_seq_length:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            generated_text += \" \" + sampled_word\n",
        "        \n",
        "        # Update the target sequence\n",
        "        target_sequence = np.zeros((1, max_seq_length))\n",
        "        target_sequence[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "    \n",
        "    return generated_text.strip()\n",
        "\n",
        "# Create the test dataset with generated texts\n",
        "def create_final_dataset(test_dataset, tokenizer, encoder_model, decoder_model, max_seq_length):\n",
        "    test_dataset['generated_text'] = test_dataset['Review'].apply(\n",
        "        lambda x: generate_text(x, tokenizer, encoder_model, decoder_model, max_seq_length)\n",
        "    )\n",
        "    return test_dataset\n",
        "\n",
        "# Assume that you already have the model trained and the variables prepared\n",
        "# test_dataset: Preprocessed test dataset\n",
        "final_test_dataset = create_final_dataset(test_dataset, tokenizer, model, model, max_seq_length)\n",
        "\n",
        "# Save the final dataset\n",
        "final_test_dataset.to_csv(\"final_test_dataset_with_generated_text.csv\", index=False)\n",
        "print(\"Archivo final generado: 'final_test_dataset_with_generated_text.csv'\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
